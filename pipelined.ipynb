{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import date, timedelta  \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "import os\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.read_feather('C:/Users/asus/Documents/rts27.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader: \n",
    "\n",
    "    def drop_cols(self, data):\n",
    "        data.drop(['file_name','designated_market_makers','legal_entity_name', 'systematic_internaliser_flag'], axis = 1, inplace = True, errors='ignore')\n",
    "        return data\n",
    "    \n",
    "    def concat_tables(self):\n",
    "        dfArray = []\n",
    "        dfArray.append(pd.read_csv('C:/Users/asus/Documents/RTS27 data/RTS27_NWMS_Table2_FinancialInstrumentIdentification_20180103_20180331.csv'))\n",
    "        dfArray.append(pd.read_csv('C:/Users/asus/Documents/RTS27 data/RTS27_NWMS_Table2_FinancialInstrumentIdentification_20180401_20180630.csv'))\n",
    "        dfArray.append(pd.read_csv('C:/Users/asus/Documents/RTS27 data/RTS27_NWMS_Table2_FinancialInstrumentIdentification_20180701_20180930.csv'))\n",
    "        dfArray.append(pd.read_csv('C:/Users/asus/Documents/RTS27 data/RTS27_NWMS_Table2_FinancialInstrumentIdentification_20181001_20181231.csv'))\n",
    "        dfArray.append(pd.read_csv('C:/Users/asus/Documents/RTS27 data/RTS27_NWMS_Table2_FinancialInstrumentIdentification_20190101_20190331.csv'))\n",
    "        newData = pd.concat(dfArray)\n",
    "        return newData\n",
    "\n",
    "    def modify_table_data(self, data):\n",
    "        newData = self.concat_tables()\n",
    "        newData = newData.rename(columns={'Financial Instrument Name':'source_instrument_name', 'Instrument Identifier(ISO 6166)':'instrument_identifier'})\n",
    "        newData = newData[['source_instrument_name','instrument_identifier']].dropna()\n",
    "        newData.drop_duplicates(subset ='instrument_identifier', keep = 'first', inplace = True) \n",
    "        data['source_instrument_name'] = data['instrument_identifier'].map(newData.set_index('instrument_identifier')['source_instrument_name'])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def filter_fx_fwrds(self, data):\n",
    "        #Filters the code to FX forwards only\n",
    "        dropped_data = self.drop_cols(data)\n",
    "        flt = dropped_data.loc[(data['instrument_classification']).astype(str).str.contains(\"JF\") | (data['source_instrument_desc']).astype(str).str.contains(\"Forward\") | (data['source_instrument_name']).astype(str).str.contains(\"Forward\")]\n",
    "        return flt\n",
    "    \n",
    "    def get_bank(self, bank_name, data):\n",
    "        banks = data.groupby(data['bank_name'])\n",
    "        return banks.get_group(bank_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardsCleaner:\n",
    "    \n",
    "    def fill_trans_values(self, df):\n",
    "        df['total_value_of_trans_gbp'] = df['total_value_of_trans_gbp'].fillna(df['total_value_of_trans_eur']).fillna(df['total_value_of_transactions'])\n",
    "        return df\n",
    "    \n",
    "    def get_IMM_rolls(self, start_date, end_date, data, imm_date):\n",
    "        rtsQ = data.loc[(data['date_of_trading_day'] > start_date) & (data['date_of_trading_day'] <= end_date) & ((data['source_instrument_desc']).astype(str).str.contains(imm_date) | (data['source_instrument_name']).astype(str).str.contains(imm_date))]\n",
    "        return rtsQ\n",
    "    \n",
    "    def get_roll_quarters(self, dates, data):\n",
    "        quarters = []\n",
    "        for q in dates:\n",
    "            quarters.append(self.get_IMM_rolls(q[0], q[1], data, q[2]))\n",
    "        return quarters\n",
    "    \n",
    "    def get_all_quarters(self, dates, data):\n",
    "        quarters = self.get_roll_quarters(dates, data)\n",
    "        full_imm = pd.concat(quarters)\n",
    "        return full_imm\n",
    "    \n",
    "    def get_bank_rolls(self, dates, bank, data):\n",
    "        rolls = self.get_roll_quarters(dates, reducedRTS)\n",
    "        bank_rolls = []\n",
    "        for q in rolls:\n",
    "            bank_rolls.append(data_reader.get_bank(bank, q))\n",
    "        return bank_rolls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPipeline:\n",
    "    \n",
    "    def predict(self, data, target, m):\n",
    "        # Select target\n",
    "        y = data[target]\n",
    "        # To keep things simple, we'll use only numerical predictors\n",
    "        X = data.drop([target], axis=1)\n",
    "\n",
    "        im = SimpleImputer(strategy='mean',missing_values=np.nan)\n",
    "        y = y.to_numpy().reshape(-1,1)\n",
    "        y = im.fit_transform(y)\n",
    "        y = y.flatten()\n",
    "\n",
    "                # Divide data into training and validation subsets\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                              random_state=0)\n",
    "\n",
    "        categorical_cols = [col for col in X.columns if X[col].dtype in ['object']]\n",
    "\n",
    "        # Select numerical columns\n",
    "        numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "        # Keep selected columns only\n",
    "        my_cols = categorical_cols + numerical_cols\n",
    "        X_train_filtered = X_train[my_cols].copy()\n",
    "        X_valid_filtered = X_valid[my_cols].copy()\n",
    "\n",
    "        numerical_transformer = SimpleImputer(strategy='mean',missing_values=np.nan)\n",
    "\n",
    "        # Preprocessing for categorical data\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer',SimpleImputer(strategy='most_frequent',missing_values=None)),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "\n",
    "        sc = StandardScaler(with_mean=False)\n",
    "\n",
    "        # Bundle preprocessing for numerical and categorical data\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, numerical_cols),\n",
    "                ('cat', categorical_transformer, categorical_cols)\n",
    "            ], remainder='passthrough')\n",
    "\n",
    "        model =  m\n",
    "\n",
    "        svd = TruncatedSVD(n_components=5)\n",
    "\n",
    "        # Bundle preprocessing and modeling code in a pipeline\n",
    "        my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('scaler', sc),\n",
    "                                      ('svd', svd) ,\n",
    "                                      ('model', model)\n",
    "                                     ])\n",
    "\n",
    "        # Preprocessing of training data, fit model \n",
    "        my_pipeline.fit(X_train_filtered, y_train)\n",
    "\n",
    "        # Preprocessing of validation data, get predictions\n",
    "        preds = my_pipeline.predict(X_valid_filtered)\n",
    "\n",
    "        return preds.flatten(), X_valid, y_valid\n",
    "    \n",
    "    \n",
    "    def plot_preds(self, data, target, m):\n",
    "        preds = self.predict(data, target, m)[0]\n",
    "        x_val = self.predict(data, target, m)[1]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize = (12,6))\n",
    "        sns.lineplot(x = x_val['date_of_trading_day'], y = preds, estimator = sum, ci = None ,ax=ax, label='predicted')\n",
    "        #fig, ax2 = plt.subplots(figsize = (12,6))\n",
    "        sns.lineplot(x = x_val['date_of_trading_day'], y = data['total_value_of_trans_gbp'], estimator = sum, ci = None ,ax=ax, label='actual')\n",
    "\n",
    "        x_dates = data['date_of_trading_day'] .dt.strftime('%Y-%m-%d').sort_values().unique()\n",
    "        ax.set_xticklabels(labels=x_dates, rotation=45, ha='right')\n",
    "        #ax2.set_xticklabels(labels=x_dates, rotation=45, ha='right')\n",
    "        \n",
    "    def get_error(self, data, target, m):\n",
    "        preds = self.predict(data, target, m)[0]\n",
    "        y_val = self.predict(data, target, m)[2]\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        return(mae)\n",
    "    \n",
    "    \n",
    "    def get_arima_train_test(self, rolls):\n",
    "        #Split data into univariate\n",
    "        clean_rolls = []\n",
    "        for q in rolls:\n",
    "            clean_q = q.drop(q.columns.difference(['date_of_trading_day', 'total_value_of_trans_gbp']), 1)\n",
    "            clean_q['date_of_trading_day'] = pd.to_datetime(q['date_of_trading_day'])\n",
    "            clean_q.set_index('date_of_trading_day', inplace= True)\n",
    "            clean_rolls.append(clean_q)\n",
    "\n",
    "        train = pd.concat([clean_rolls[0], clean_rolls[1]])\n",
    "        test = clean_rolls[2]\n",
    "        train.fillna(train.mean(), inplace=True)\n",
    "        test.fillna(test.mean(), inplace=True)\n",
    "        \n",
    "        return train, test\n",
    "    \n",
    "    def auto_arima_predict(self, rolls, model):\n",
    "        train, test = self.get_arima_train_test(rolls)\n",
    "        prediction = pd.DataFrame(model.predict(n_periods = test.shape[0]),index=test.index)\n",
    "        return prediction\n",
    "    \n",
    "    def plot_auto_arima(self, rolls, model, bank):\n",
    "        prediction = self.auto_arima_predict(rolls, model)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize = (12,6))\n",
    "        sns.lineplot(x = prediction.index, y= prediction.iloc[:,0], estimator = sum, ci = None ,ax=ax, label='training').set_title(bank + \": Time series forecasting of total value of transactions\")\n",
    "        sns.lineplot(x = test.index, y= test.iloc[:,0], estimator = sum, ci = None ,ax=ax, label='actual')\n",
    "        sns.lineplot(x = train.index, y= train.iloc[:,0], estimator = sum, ci = None ,ax=ax, label='predicted')\n",
    "        \n",
    "        fig, ax2 = plt.subplots(figsize = (12,6))\n",
    "        sns.lineplot(x = prediction.index, y= prediction.iloc[:,0], estimator = sum, ci = None ,ax=ax2, label='predicted')\n",
    "        sns.lineplot(x = test.index, y= test.iloc[:,0], estimator = sum, ci = None ,ax=ax2, label='actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-94c622fc750f>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['total_value_of_trans_gbp'] = df['total_value_of_trans_gbp'].fillna(df['total_value_of_trans_eur']).fillna(df['total_value_of_transactions'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bank_name</th>\n",
       "      <th>date_of_trading_day</th>\n",
       "      <th>instrument_identifier</th>\n",
       "      <th>instrument_classification</th>\n",
       "      <th>order_request_quotes_received</th>\n",
       "      <th>executed_transactions</th>\n",
       "      <th>total_value_of_transactions</th>\n",
       "      <th>order_request_quotes_can_wdrwn</th>\n",
       "      <th>order_request_quotes_modified</th>\n",
       "      <th>...</th>\n",
       "      <th>median_transaction_size_eur</th>\n",
       "      <th>median_transaction_size_gbp</th>\n",
       "      <th>median_size_request_quotes_eur</th>\n",
       "      <th>median_size_request_quotes_gbp</th>\n",
       "      <th>smartie_instrument_name</th>\n",
       "      <th>data_quality_flag</th>\n",
       "      <th>egb_eligible_flag</th>\n",
       "      <th>year_quarter</th>\n",
       "      <th>business</th>\n",
       "      <th>central_bank_isin_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>7201509</td>\n",
       "      <td>HSBC</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>EZ4BH61JGHV3</td>\n",
       "      <td>None</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.141891e+06</td>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018 Q1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7888</th>\n",
       "      <td>7173565</td>\n",
       "      <td>HSBC</td>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>EZ97W6R9R8B9</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>43659.009168</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>4.365901e+04</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018 Q1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9723</th>\n",
       "      <td>6679050</td>\n",
       "      <td>HSBC</td>\n",
       "      <td>2018-03-16</td>\n",
       "      <td>EZ2L6TGX76N5</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018 Q1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9743</th>\n",
       "      <td>6679074</td>\n",
       "      <td>HSBC</td>\n",
       "      <td>2018-03-16</td>\n",
       "      <td>EZ3T5W5YBHM6</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018 Q1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12483</th>\n",
       "      <td>6678481</td>\n",
       "      <td>HSBC</td>\n",
       "      <td>2018-03-14</td>\n",
       "      <td>EZ51B42LC8K0</td>\n",
       "      <td>None</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.247768e+07</td>\n",
       "      <td>1.992826e+07</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018 Q1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id bank_name date_of_trading_day instrument_identifier  \\\n",
       "5219   7201509      HSBC          2018-03-28          EZ4BH61JGHV3   \n",
       "7888   7173565      HSBC          2018-03-23          EZ97W6R9R8B9   \n",
       "9723   6679050      HSBC          2018-03-16          EZ2L6TGX76N5   \n",
       "9743   6679074      HSBC          2018-03-16          EZ3T5W5YBHM6   \n",
       "12483  6678481      HSBC          2018-03-14          EZ51B42LC8K0   \n",
       "\n",
       "      instrument_classification  order_request_quotes_received  \\\n",
       "5219                       None                            6.0   \n",
       "7888                       None                            2.0   \n",
       "9723                       None                            1.0   \n",
       "9743                       None                            1.0   \n",
       "12483                      None                            6.0   \n",
       "\n",
       "       executed_transactions  total_value_of_transactions  \\\n",
       "5219                     0.0                          0.0   \n",
       "7888                     2.0                     100000.0   \n",
       "9723                     0.0                          0.0   \n",
       "9743                     0.0                          0.0   \n",
       "12483                    0.0                          0.0   \n",
       "\n",
       "       order_request_quotes_can_wdrwn  order_request_quotes_modified  ...  \\\n",
       "5219                              6.0                            0.0  ...   \n",
       "7888                              0.0                            0.0  ...   \n",
       "9723                              0.0                            0.0  ...   \n",
       "9743                              1.0                            0.0  ...   \n",
       "12483                             6.0                            0.0  ...   \n",
       "\n",
       "       median_transaction_size_eur  median_transaction_size_gbp  \\\n",
       "5219                           0.0                     0.000000   \n",
       "7888                       50000.0                 43659.009168   \n",
       "9723                           0.0                     0.000000   \n",
       "9743                           0.0                     0.000000   \n",
       "12483                          0.0                     0.000000   \n",
       "\n",
       "      median_size_request_quotes_eur median_size_request_quotes_gbp  \\\n",
       "5219                    1.141891e+06                   1.000000e+06   \n",
       "7888                    5.000000e+04                   4.365901e+04   \n",
       "9723                    0.000000e+00                   0.000000e+00   \n",
       "9743                    0.000000e+00                   0.000000e+00   \n",
       "12483                   2.247768e+07                   1.992826e+07   \n",
       "\n",
       "      smartie_instrument_name data_quality_flag egb_eligible_flag  \\\n",
       "5219                     None              None              None   \n",
       "7888                     None              None              None   \n",
       "9723                     None              None              None   \n",
       "9743                     None              None              None   \n",
       "12483                    None              None              None   \n",
       "\n",
       "      year_quarter business central_bank_isin_flag  \n",
       "5219       2018 Q1     None                   None  \n",
       "7888       2018 Q1     None                   None  \n",
       "9723       2018 Q1     None                   None  \n",
       "9743       2018 Q1     None                   None  \n",
       "12483      2018 Q1     None                   None  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reader = DataReader()\n",
    "fwrds_cleaner = ForwardsCleaner()\n",
    "\n",
    "merged_data = data_reader.modify_table_data(df)\n",
    "fx_fwrds = data_reader.filter_fx_fwrds(merged_data)\n",
    "clean_fx_fwrds = fwrds_cleaner.fill_trans_values(fx_fwrds)\n",
    "dates = [['2018-03-07', '2018-03-28',\"20180620\"], \n",
    "         ['2018-06-06', '2018-06-27', \"20180919\"],\n",
    "         ['2018-09-05', '2018-09-26', \"20181219\"],\n",
    "         ['2018-12-05', '2018-12-26', \"20190320\"]]\n",
    "\n",
    "fx_rolls = fwrds_cleaner.get_all_quarters(dates, clean_fx_fwrds)\n",
    "fx_rolls.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_rolls.reset_index(drop=True).to_feather('C:/Users/asus/Documents/RTS27filtered.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedRTS = pd.read_feather(\"C:/Users/asus/Documents/RTS27filtered.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Credit Suisse due to missing data\n",
    "reducedRTS = reducedRTS.drop(reducedRTS.groupby('bank_name').get_group('Credit Suisse').index)\n",
    "reducedRTS = reducedRTS.drop(reducedRTS.groupby('bank_name').get_group('UBS').index)\n",
    "reducedRTS = reducedRTS.drop(reducedRTS.groupby('bank_name').get_group('Barclays').index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:963: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "C:\\Users\\asus\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:975: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "C:\\Users\\asus\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:963: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "C:\\Users\\asus\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:975: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n"
     ]
    }
   ],
   "source": [
    "f_cleaner = ForwardsCleaner()\n",
    "#q1 = fwrds_cleaner.get_roll_quarters(dates, reducedRTS)[1]\n",
    "pipeline = MyPipeline()\n",
    "#print(fwrds_cleaner.get_roll_quarters(dates, reducedRTS)[1].shape)\n",
    "\n",
    "for bank, group in reducedRTS.groupby('bank_name'):\n",
    "    bank_rolls = f_cleaner.get_bank_rolls(dates, bank, reducedRTS)\n",
    "    train, test = pipeline.get_arima_train_test(bank_rolls)\n",
    "    model = auto_arima(train,start_p=0, d=None, start_q=0, \n",
    "                              max_p=20, max_d=20, max_q=20, start_P=0, \n",
    "                              D=None, start_Q=0, max_P=0, max_D=0,\n",
    "                              max_Q=0, m=1, seasonal=False, \n",
    "                              error_action='warn',trace = False,\n",
    "                              supress_warnings=True,stepwise = True,\n",
    "                              random_state=20,n_fits = 100 )\n",
    "\n",
    "    pipeline.plot_auto_arima(bank_rolls, model, bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "q1 = fwrds_cleaner.get_roll_quarters(dates, reducedRTS)[1]\n",
    "banks = q1.groupby(reducedRTS['bank_name'])\n",
    "\n",
    "for i, bank in banks:\n",
    "    fig, ax = plt.subplots(figsize = (12,6))\n",
    "    sns.barplot(x = bank['date_of_trading_day'], y = bank['total_value_of_trans_gbp'], estimator = sum, ci = None ,ax=ax, hue = bank['bank_name'])\n",
    "    x_dates = q1['date_of_trading_day'] .dt.strftime('%Y-%m-%d').sort_values().unique()\n",
    "    ax.set_xticklabels(labels=x_dates, rotation=45, ha='right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\", color_codes=True) \n",
    "\n",
    "g = sns.pairplot(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSBC_Q1 = data_reader.get_bank('HSBC',q1)\n",
    "HSBC_Q1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.flatten()\n",
    "print(preds.shape)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,6))\n",
    "sns.barplot(x = X_valid['date_of_trading_day'], y = preds, estimator = sum, ci = None ,ax=ax)\n",
    "fig, ax2 = plt.subplots(figsize = (12,6))\n",
    "sns.barplot(x = X_valid['date_of_trading_day'], y = HSBC_Q1['total_value_of_trans_gbp'], estimator = sum, ci = None ,ax=ax2)\n",
    "\n",
    "x_dates = q1['date_of_trading_day'] .dt.strftime('%Y-%m-%d').sort_values().unique()\n",
    "ax.set_xticklabels(labels=x_dates, rotation=45, ha='right')\n",
    "ax2.set_xticklabels(labels=x_dates, rotation=45, ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTDATA_FILENAME = 'C:/Users/inald/Documents/NWMprod/wetransfer-0f4d76/rts27.feather'\n",
    "dfTest = pd.read_feather('C:/Users/inald/Documents/NWMprod/wetransfer-0f4d76/rts27.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNotebook(unittest.TestCase):\n",
    "    \n",
    "    \n",
    "    def test_add(self):\n",
    "        self.assertEqual(4,4)\n",
    "\n",
    "    #checks that the dataframes contain frames\n",
    "    def test_data_read(self):\n",
    "        if dfTest.empty:\n",
    "            self.assertTrue(False)\n",
    "        else:\n",
    "            self.assertTrue(True)\n",
    "    \n",
    "    #Test to check that columns are disposed of that are not needed        \n",
    "    def test_dropping_columns(self):\n",
    "        dropped_df = dfTest.drop(['date_created', 'instrument_identifier'], axis=1)\n",
    "        Xtest = dropped_df\n",
    "        count = 0\n",
    "        count2 = 0\n",
    "        for col in dfTest.columns:\n",
    "            count = count + 1\n",
    "        for col in Xtest.columns:\n",
    "            count2 = count + 1\n",
    "        self.assertNotEqual(count, count2)\n",
    "        \n",
    "   #Test to check that the train data is larger than the test - test the 80% v 20% difference    \n",
    "    def test_splitting_data_size(self):\n",
    "        Xtest = dfTest.drop(['date_created', 'instrument_identifier'], axis=1)\n",
    "        ytest = dfTest['date_created']\n",
    "        Xtest_train, Xtest_valid, ytest_train, ytest_valid = train_test_split(Xtest, ytest, train_size=0.8, test_size=0.2, random_state=0)\n",
    "        if Xtest_train.size > Xtest_valid.size:\n",
    "            self.assertTrue(True)\n",
    "        else:\n",
    "            self.assertTrue(False)\n",
    "    \n",
    "    unittest.main(argv=[''], verbosity=3, exit=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
